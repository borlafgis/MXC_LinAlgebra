# Vectors

```{python}
import numpy as np
import matplotlib.pyplot as plt
```


## Algebraic and geometric interpretations

Many concepts in linear algebra can be conceptualized using both algebra and
geometry. This is convenient because some concepts are more intuitive from one
of these two points of view.

A vector is an ordered list of numbers like this. Their representation generally
is surrounded by brackets (sometimes angled brackets, or parentheses). Vectors
contain several elements, numbers of any kind: integers or fractions,
real or complex, rational or irrational. 

$$
\begin{bmatrix}
1 \\ 2 \\ 3
\end{bmatrix}
\neq
\begin{bmatrix}
2 \\ 1 \\ 3
\end{bmatrix}
$${#eq-vec-order}

The number of elements in a vector is called the dimensionality of the vector.
In @eq-vec-order, both sides are three dimensional vectors. They are not the
same because the order of these elements is meaningful: the first element on
the left side is $1$, whereas on the right side it is $2$. @eq-vec-size shows
a 2- and 5-dimensional vectors.

$$
\begin{bmatrix}
3 \\ 4
\end{bmatrix}
\quad
\begin{bmatrix}
1 \\ 0 \\ \pi \\ 5i \\ -2
\end{bmatrix}
$${#eq-vec-size}

Vectors can be vertical, case where they are called column vectors, or
horizontal, case where they are called row vectors.

$$
\begin{bmatrix}
1 & 2 & 3 & 5 & 4 \\
\end{bmatrix}
$$

Nomenclature around vectors is a bit confusing because source it uses one of
many conventions. The most common way to refer to vectors is using a lowercase,
bold Latin letter, such as $\mathbf{v}$. Sometimes you'll see a little arrow on top of
the letter $\left( \vec{\mathbf{v}} \right)$, which is used to avoid ambiguity about
whether or not this is referring to a vector. Rarely you may encounter vectors
represented using italicized letters. Here we just use lowercase boldface
letters $\left( \mathbf{v} \right)$.

### Geometric interpretation

In geometric terms a vector represents a straight line with some length and
some direction, which is specified by the numbers contained in the vector.
The starting point of vector is called the tail of the vector, and its end is
called the head, symbolized by an arrowhead. 

Note a vector is defined just by its length and direction respective from the
start: it these could start from any point in space. In other words, vectors
are robust to translation Â¿or rigid body motion?. Thus, even though it may be
tempting to interpret the contents of a 2D vector as a coordinate, this only
holds if the tail of the vector is placed on the origin: the vector and the
coordinates are overlapping. If a vector starts at $(0, 0)$ they are considered
to be in standard position. Such position is convenient for many reasons (it
can be assumed if nothing has stated otherwise).

A vector can be 2-, 3-dimensional, etc. Even though past this point
visualization becomes challenging, the principle remains the same.
It also shows a common theme in linear algebra: the geometric interpretation
is useful up to 3 dimensions, point where is more intuitive to switch back
to algebra.

Note vectors can also contain functions.

$$
\begin{bmatrix}
\sin (x) \\
x \cos(x) \\
x \\
\end{bmatrix}
$${#eq-fun-vector}

@eq-fun-vector is a vector valued function where $x$ is some continuous vector.
However, in this course, I'm only going to be talking about vectors where each
element is a number, not an entire function.

### Python

We will be using Numpy, Matplotlib and their submodules (pyplot) for all the
lessons in this course. A list could be considered a 2D vector, but in most
cases is preferable to use a Numpy array, specially for complex cases.
Transpose swaps, the rows and the columns. This operation can be called as a
function, or as a method of Numpy arrays (`.T`).

When we use the function `plt.plot` we are just specifying the $x$ and the $y$
values to draw a line. The code indicates the line start at the origin
$(0, 0)$, and ends at the $x$, $y$ locations corresponding to the first and
second elements of the vector, respectively. So the $x$ coordinate is at $3$
and the $y$ coordinate is $-2$. The rest of the code sets the aspect of the
figure, adds the cartesian axis lines, the grid and sets the plot extent.

```{python}

# 2-dimensional vector
v2 = [ 3, -2 ]

# 3-dimensional vector
v3 = [ 4, -3, 2 ]

# row to column (or vice-versa):
v3t = np.transpose(v3)


# plot them
plt.plot([0,v2[0]],[0,v2[1]])
plt.axis('equal')
plt.plot([-4, 4],[0, 0],'k--')
plt.plot([0, 0],[-4, 4],'k--')
plt.grid()
plt.axis((-4, 4, -4, 4))
```

3D plotting in Python requires a few actual pieces of code compared to plotting
in 2D. In this case, `ax.plot` is a method of a figure panel (`ax` in
Matplotlib terminology) which has been been set to be projected in 3D.
The first argument are the $x$ coordinates, the second the $y$ coordinates,
ad the third the $z$ coordinates. The rest of the code is just setting the axis,
the extent, etc.

```{python}

# plot the 3D vector
fig = plt.figure(figsize=plt.figaspect(1))
# ax = fig.add_subplot(projection='3d')
ax = plt.axes(projection = '3d')
ax.plot([0, v3[0]],[0, v3[1]],[0, v3[2]],linewidth=3)

# make the plot look nicer
ax.plot([0, 0],[0, 0],[-4, 4],'k--')
ax.plot([0, 0],[-4, 4],[0, 0],'k--')
ax.plot([-4, 4],[0, 0],[0, 0],'k--')
plt.show()
```

### Summary

* Algebraic and the geometric interpretation of a vector
* The difference between a vector and a coordinate and the one case
* The special case where the vector and the coordinate overlap when the vector
*  is being drawn in its standard position.

## Vector addition/subtraction

When doing operations with vectors it is necessary they have the same number of
elements, the same dimensionality.

### Algebraic interpretation

Algebraic addition or subtraction just apply the operation to each
pair of elements.

$$
\begin{bmatrix}
1 \\ 0 \\ 4 \\ 3
\end{bmatrix}
+
\begin{bmatrix}
2 \\ -3 \\ -2 \\ 1
\end{bmatrix}
=
\begin{bmatrix}
3 \\ -3 \\ 2 \\ 4
\end{bmatrix}
$$

### Geometric interpretation

#### Addition

Geometrically, to add two vectors, you put the tail of one vector at the head
of the other (remember, vectors are about length and orientation, not coordinates).
And just draw the line that goes from the tail of the first vector to the head
of the second.

#### Subtraction

Geometrically, vector subtraction has two interpretations.

You can think of subtraction as multiplying the second vector by $-1$, element
by element, and then adding it to the first vector. This flips the direction of
the vector.

The second way to think about vector subtraction is to put both vectors into
the standard position, and then you draw a straight line going from the head of
the negative negative vector ("$-$" sign) to the head of the positive vector.

In both cases the resulting vector could be translated back to the standard position.

In the PDF file there are more exercises so you can get more comfortable with
vector addition and subtraction, both from the algebraic and the geometric
perspectives.

**translated vector V2 so that it starts at the head of Vector V1**

### Python

Vector addition and subtraction requires the use Numpy arrays (lists are
concatenated with the plus sign). They can added or subtracted with just the
`+` or the `-` sign. You can think about the addition of two vectors as going
from the tail of one vector to the head of the other vector when that vector
is translated to be at the end of the first vector.

```{python}

# two vectors in R2
v1 = np.array([ 3, -1 ])
v2 = np.array([ 2,  4 ])

v3 = v1 + v2

# plot them
plt.plot([0, v1[0]],[0, v1[1]],'b',label='v1')
plt.plot([0, v2[0]]+v1[0],[0, v2[1]]+v1[1],'r',label='v2')
plt.plot([0, v3[0]],[0, v3[1]],'k',label='v1+v2')

plt.legend()
plt.axis('square')
plt.axis((-6, 6, -6, 6))
plt.grid()
plt.show()
```

## Vector-scalar multiplication

A scalar is just a single number outside of a vector or a matrix.

*And you'll see in a few minutes why single numbers in linear algebra are called scalars.*

In linear algebra symbology vectors are represented with boldfaced lowercase
Latin letters (e.g., $\mathbf{v}$), matrices with capital boldface letters
(e.g., $\mathbf{M}$), and scalars with lowercase non boldfaced Greek letters
(e.g., $\alpha$). Note this convention is not followed all the time.

$$
\textbf{v}
=
\begin{bmatrix}
-1 \\ 0 \\ 1
\end{bmatrix}
\quad
\textbf{M}
=
\begin{bmatrix}
-1 & 0 & 2 & 8 \\
0 & 1 & 4 & 4 \\
1 & 4 & 9 & 1 \\
\end{bmatrix}
\quad
\alpha = 7
$$

Vector scalar multiplication has both an algebraic and a geometric interpretation.

### Algebraic

Algebraically vector scalar multiplication means to multiply each element by
the scalar.

$$
\lambda \mathbf{v}
=
7
\begin{bmatrix}
-1 \\ 0 \\ 1
\end{bmatrix}
=
\begin{bmatrix}
7 \times -1 \\
7 \times 0 \\
7 \times 1
\end{bmatrix}
=
\begin{bmatrix}
-7 \\ 0 \\ 7
\end{bmatrix}
$$

### Geometric

The geometric interpretation of vector scalar multiplication is to "scale"
the vector according to the scale (no change in direction). If the scalar is
greater than one ($\lambda > 1$), the resulting vector will be longer than the
original, whereas if it is $\lambda \in (0, 1)$, the result will be shorter.
If the scalar is negative ($\lambda < 0$) spins around and points in the other
way (same direction, opposite orientation).

*[The vector line] is actually lying on a one dimensional subspace that goes
infinitely long in [either] direction. And in that sense, which is a more
linear algebra sense, this vector is not actually pointing in a different
direction with regards to the one dimensional subspace, the infinitely long
line that this line or this vector is on. If scaling a vector doesn't change
its angle, that means that all vectors obtaining by scaling one particular
vector are related to each other in a special way: they lie on the same
subspace (see later).*

### Python 

We create a 2D vector `v1` and multiply by `l`, standing for lambda, $\lambda$.
`l = -.3` is negative and its magnitude is smaller than one, which means the
result will be shorter than the original and have the opposite orientation even
though the direction remains the same. with `l = 2.3` the scaled will be longer
and follow the same orientation as the original one.

The original vector is plotted in blue and the scaled version is plotted in red.
The auxiliary variable `axlim` is used to adjust the extent of the plot
according to largest elements in the original and the scaled vectors.

```python
# vector and scalar
v1 = np.array([ 3, -1 ])
l  = 2.3
v1m = v1*l # scalar-modulated

# plot them
plt.plot([0, v1[0]],[0, v1[1]],'b',label='$v_1$')
plt.plot([0, v1m[0]],[0, v1m[1]],'r:',label='$\lambda v_1$')

plt.legend()
plt.axis('square')
axlim = max([max(abs(v1)),max(abs(v1m))])*1.5 # dynamic axis lim
plt.axis((-axlim,axlim,-axlim,axlim))
plt.grid()
plt.show()
```

### Summary

Multiplying a vector by a scalar changes the length but preserves the direction
of that vector (not the orientation). It may seem like trivial operation, but
this idea turns out to be the fundamental insight that leads to eigenvalue
decomposition, one of the most important applications of linear algebra.

# Vector-vector multiplication: the dot product

There are four ways to multiply two vectors:

1. Had Ahmad multiplication, t
2. The dot product, 
3. the cross product
4. the outer product

The dot product is a single number that provides information about the
relationship between two vectors with the same dimensionality. Sometimes it
also is called the scalar product because the result is a single number.
The dot product is the computational cornerstone of many algorithms, such as
convolution, correlation or the Fourier transform.

### Algebraic interpretation (only)

The DOT product is indicated using several different notations.

$$
\alpha
= \mathbf{a} Â· \mathbf{b} = \langle \mathbf{a}, \mathbf{b} \rangle
= \mathbf{a}^{T} \mathbf{b} = \sum^{n}_{i=1} a_i b_i
$$

The dot product is a scalar, so you can refer to its result using a lowercase
greek letters. You also may see a dot between the two vectors or wide angle
brackets. The latter is more commonly used for continuous functions, but
sometimes it is used used for the dot product. $a^{T} b$ notation is the most
common in linear algebra: the first vector transpose $(T)$ times, the second
vector.

$\mathbf{a}^T \mathbf{b}$ indicates the vector dot product assuming
$\mathbf{a}$ and $\mathbf{b}$ are both column vectors. The algebraic
definition of the dot product is just element-wise and sum the results.
Let me unpack this formula by showing you an example. @eq-dot-ew shows the
dot product between two five-dimensional column vectors, $\mathbf{v}$ and
$\mathbf{v}$. The dot product between these two vectors is element-wise
multiplication and then sum.

$$
v=
\begin{bmatrix}
1 \\ 0 \\ 2 \\ 5 \\ -2
\end{bmatrix}
\quad
w=
\begin{bmatrix}
2 \\ 8 \\ -6 \\ 1 \\ 0
\end{bmatrix}
\quad
\mathbf{v}^T \mathbf{w} =
1 \times 2 + 0 \times 8 + 2 \times (-6) + 5 \times 1 + (-2) \times 0 = -5
$${#eq-dot-ew}

### Python

The first step is to create the vectors `v1`and `v2`.

```{python}
v1 = np.array([ 1, 2, 3, 4, 5 ]) # , 6
v2 = np.array([ 0, -4, -3, 6, 5 ])
```

On the barest terms, we could  loop through the elements of the vectors,
multiply the $i^{th}$ element of the vectors, and add to a variable initialized
at $0$.

```{python}
the_sum = 0
for i in range(len(v1)):
    the_sum += v1[i] * v2[i]
print(the_sum)
```

This code can be reduced a bit using Numpy's vectorized operations

```{python}
print(np.sum(np.multiply(v1,v2)), np.sum(v1 * v2))
```

The library also has the specialized function `np.dot` to calculate the dot
product.

```{python}
print(np.dot(v1, v2))
```

It is also possible to calculate the dot product using matrix multiplication
(see next sections).

```{python}
print(np.matmul( v1,v2 ), v1 @ v2)
```

Even though the result is $32$ in all cases, in practice is advisable to use
`np.dot` function for convenience, clarity and speed.

# VIDEO: Dot product properties: associative and distributive

The goal of this video is to explore the associative property and the
distributive properties of the dot product.

### The distributive property.

When working with scalars the distributive property means that $a(b+c)$ is the
same thing as $ab + ac$

$$
a (b + c) = ab + bc
$$

When working with vectors

$$
\mathbf{a}^{T}(\mathbf{b} + \mathbf{c})
=
\mathbf{a}^{T}\mathbf{b} + \mathbf{a}^{T}\mathbf{c}
$$

Thus, the vector dot product is distributive.

To explain why that is the case, I'm going to actually start from the
rightmost term and rewrite its two DOT products.

$$
\sum^{n}_{i=1} a_i b_i + \sum^{n}_{i=1} a_i c_i
=
\sum^{n}_{i=1} a_i (b_i + c_i)
$$

Using the algebraic definition, is the element-wise multiplication.
So we take elements $i$ to $n$ (dimensionality of the vectors) from each vector,
multiply one another, and sum. All three vectors have the same dimensionality
and all the operations are linear (multiplication/summation).
Thus, it is possible to combine the Â¿like? terms and rewrite:
the vector dot product is distributive.

### Associative property

When working with the associative property indicating the arrangement of the
parentheses does not alter the result.

$$
a(b \times c) = (a \times b) c
$$

However, the vector dot product is **not** associative (with some exceptions).

$$
\mathbf{a}^{T}(\underbrace{\mathbf{b}^{T}\mathbf{c}}_{scalar})
\neq
(\underbrace{\mathbf{a}^{T}\mathbf{b}}_{scalar})^{T}\mathbf{c}
$$

#### Why not? (a)

Let's assume that $\mathbf{a}$, $\mathbf{b}$ and $\mathbf{c}$ are all distinct
vectors, and none of them is a zeros vector (all $0$'s). The contents of the
parentheses are both dot products, whose result is going to be a pair of
scalars. If, as we said all three vectors are different, we can expect the
contents of the parentheses to be different. Not only that, $\mathbf{a}^{T}$
and $\mathbf{c}$ are have different orientations (row vs. column or
*vice versa*) and are multiplied by different scalars.

#### Why not? (b)

Another explanation comes from dimensionality of the different vectors.
If vector $\mathbf{a}$ has $5$ elements
$\left( \mathbf{a} \in \mathbb{R}^{5} \right)$, anc vectors $\mathbf{b}$ and
$\mathbf{c}$ have three elements, $(\mathbf{b}, \mathbf{c}) \in \mathbb{R}^{3}$.
In this context, $\mathbf{a}^{T}(\mathbf{b}^{T}\mathbf{c})$ would be a valid
expression because $\mathbf{b}$ and $\mathbf{c}$ have the same dimensionality.
Thus, it boils down to a simple vector-scalar multiplication. However,
$(\mathbf{a}^{T}\mathbf{b})^{T}\mathbf{c}$ would not be a valid multiplication
$\mathbf{a}$ and $\mathbf{b}$ have different numbers of elements.

#### Special cases

The associative property only holds if one the vectors is a zeros vector, or if
the elements all three are the same.  

#### Pitfall

Associative property does not hold for the dot product is not associative, but
it does foe matrix multiplication. In fact, this associative property for
matrix multiplications turns out to be really crucial for quite
a few proofs in advanced linear algebra (see next sections).

$$
\mathbf{A}(\mathbf{B}\mathbf{C}) = (\mathbf{A}\mathbf{B})\mathbf{C}
$$

### Commutative property

(See in code challenge)

### Python

#### Distributive property

The first step is to create three random vectors with $10$ elements each. These
elements are drawn from a normal distribution. `res1` is the dot
product between vector $\mathbf{a}$ and vector $\mathbf{b}+\mathbf{c}$. `res2`
takes the dot product between vectors $\mathbf{a}$ and $\mathbf{b}$, and
adds it to the dot product of $\mathbf{a}$ and $\mathbf{c}$. We are distributing
the dot product from $\mathbf{a}$ to $\mathbf{b}$. When we print the numbers
the result is just the same.


```{python}
## Distributive property

# create random vectors
n = 10
a = np.random.randn(n)
b = np.random.randn(n)
c = np.random.randn(n)

# the two results
res1 = np.dot( a , (b+c) )
res2 = np.dot(a,b) + np.dot(a,c)

# compare them
print([ res1,res2 ])
```

#### Associative property

For this specific test we reduce the dimensionality `n`to ease printing out the
results. Then we compute the DOT product between vector $\mathbf{a}$ and the
dot product of $\mathbf{a}$ and $\mathbf{c}$. Note that, technically using
`np.dot` when one of the arguments is another dot product is just vector-scalar
multiplication, not a dot product as such. When we perform the calculation the
results of one and other calculation do not match. And that's because the
associative property does not actually hold.

There are two special cases where the associative property does work. The first
is when one of the vectors vector is a zeros vector. The other is when the
vectors are equal to each other $\mathbf{a} = \mathbf{b} = \mathbf{c}$.

```{python}
## Associative property

# create random vectors
n = 5
a = np.random.randn(n)
b = np.random.randn(n)
c = np.random.randn(n)

# the two results
res1 = np.dot( a , np.dot(b,c) )
res2 = np.dot( np.dot(a,b) , c )

# compare them
print(res1)
print(res2)


### special cases where associative property works!
# 1) one vector is the zeros vector
# 2) a==b==c

```

## Co. Ch.: dot product with matrix columns

Create 2 4x6 matrices of random numbers, and use a for-loop to calculate the
dot product between the corresponding columns.

```{python}
height, width = 4, 6
v1 = np.random.randn(height, width)
v2 = np.random.randn(height, width)

dots = np.full((width), np.nan)
for col_idx in range(width):
    dots[col_idx] = np.dot(v1[:, col_idx], v2[:, col_idx])

print(dots)
```

## Co. Ch. is the dot product commutative?

Generate a pair of random vectors with 100 elements each and test wether
$\mathbf{a}^{T}\mathbf{b}$ is equal to $\mathbf{b}^{T}\mathbf{a}$. Repeat the
process a couple of row vectors with two elements.

Matlab tries to stick closer to mathematics, and thus, in matlab a vector
always has an orientation. However, In Numpy we can have vectors without an
intrinsic orientation. This can induce some confusions because further abstracts
the code away from the math


The notation $\mathbf{a}^{T}\mathbf{b} == \mathbf{b}^{T}\mathbf{a}$ only is
valid when one of the vectors is a row vector and the other is a column vector,
so they match when one of them is transposed.

```{python}
long_length = 100
long1 = np.random.randn(long_length)
long2 = np.random.randn(long_length)

print(np.dot(long1, long2), np.dot(long2, long1))

short_length = 2
short1 = np.random.randn(short_length)
short2 = np.random.randn(short_length)

print(np.dot(short1, short2), np.dot(short2, short1))
```

The dot product is based on scalar multiplication. Because scalar
multiplication is commutative, the dot product is commutative as well.

Matrix multiiplication is not commutative

## Vector magnitude

Computing the length of a vector (also called the magnitude or the norm of a
vector) is pretty straightforward: the square root of the dot product of the
vector with itself. The symbol for the magnitude is these two lines at both
sides of the vector.
$$
||\mathbf{v}|| = \sqrt{ \mathbf{v}^{T} \mathbf{v} }
$$

Even thought this equation is available it is advisable to work through the
geometric interpretation of the magnitude.

If we have a 2D vector in drawn in the standard position (0, 0 of the cartesian
axis), then we can calculate its length using the Pythagoras theorem. The vector
itself is the hypothenuse, whereas the legs are its projections into the two
axis, respectively.

$$
\mathbf{v} = \begin{bmatrix} 2 \\ 3 \end{bmatrix}
\quad
||\mathbf{v}||^{2} = \left( v_{1} \right)^{2} + \left( v_{2} \right)^{2}
=
9 + 4
=
13
\quad
||\mathbf{v}|| = \sqrt{13}
$$

Applying the Pythagorean theorem is the same as taking the dot product of the
vector with itself.

$$
||\mathbf{v}||
=
\sqrt{ \mathbf{v}^{T} \mathbf{v} }
=
\sqrt{ 2 \times 2 + 3 \times 3 }
=
\sqrt{13}
$$

Note this geometric explanation only is truly intuitive for two dimensions.
However, the same principle holds for vectors of any number of dimensions,
and that's where the algebraic definition becomes really useful.

### Python

The length of a vector is just the square root of the dot product of a vector
with itself.

A possibility is to multiply the $i^{th}$ element of vector $\mathbf{v}_1$
with itself, and calculate calculate the square root of the sum of these
multiplications. The other possibility is to use `np.linalg.norm`, which
already returns the norm of te vector, its length (it is not necessary to
calculate its square root)

```{python}
# a vector
v1 = np.array([ 1, 2, 3, 4, 5, 6 ])

# methods 1-4, just like with the regular dot product, e.g.:
vl1 = np.sqrt( sum( np.multiply(v1,v1)) )

# method 5: take the norm
vl2 = np.linalg.norm(v1)

print(vl1,vl2)
```

## The dot product from a geometric perspective

```python

# two vectors
v1 = np.array([ 2,  4, -3 ])
v2 = np.array([ 0, -3, -3 ])

# compute the angle (radians) between two vectors
ang = np.arccos( np.dot(v1,v2) / (np.linalg.norm(v1)*np.linalg.norm(v2)) )


# draw them
fig = plt.figure()
# ax = fig.add_subplot(projection='3d')
ax = plt.axes(projection = '3d')
ax.plot([0, v1[0]],[0, v1[1]],[0, v1[2]],'b')
ax.plot([0, v2[0]],[0, v2[1]],[0, v2[2]],'r')

plt.axis((-6, 6, -6, 6))
plt.title('Angle between vectors: %s rad.' %ang)
plt.show()

```

In the geometric formula the DOT product ($\alpha$) between two vectors is the
cosine of the angle between those two vectors scaled by the product of their
magnitudes (their lengths, denoted by single or double lines).

$$
\alpha = |\mathbf{a}||\mathbf{b}| \cos \left( \theta_{\mathbf{ab}} \right)
$$

### How to solve for $\theta$.


Solving for Theta gives a general formula for computing the angle between two vectors. If we divide both sides by the product of the vector magnitudes we get:

$$
\frac{\alpha}{|\mathbf{a}||\mathbf{b}| }
=
\cos \left( \theta_{\mathbf{ab}} \right)
$$

Then it is just a matter of applying the arc-cosine (inverse of the cosine) to both sides of the equation.

$$
\theta_{\mathbf{ab}}
=
\arccos \left( \cos \left( \theta_{\mathbf{ab}} \right) \right)
=
\arccos \left( \frac{ \alpha }{ |\mathbf{a}||\mathbf{b}| } \right)
$$

So, the angle between two vectors ($\theta$) is the arccosine of the DOT
product between the two vectors divided by the product of their magnitudes
(their lengths).

Note even though we are plotting just the 2D case (ADD FIGURE), it works for
any number of dimensions.

Note the vector magnitudes are always going to be positive, whereas the cosine
of an angle is always going to be between $-1$ and $+1$. This fact gives us
four categories of dot product signs.

* $\theta < 90^{\circ}$, then $\cos (\theta) > 0$, and $\alpha > 0$
* $\theta > 90^{\circ}$, then $\cos (\theta) < 0$, and $\alpha < 0$
* $\theta = 90^{\circ}$, then $\cos (\theta) = 0$, and $\alpha = 0$.
  This specific case is called "orthoghonal" and is a concept that appears
  often in linear algebra.
* $\theta =  0^{\circ}$, then $\cos (\theta) = 1$, and 
  $\alpha = |\mathbf{a}||\mathbf{b}|$. Note that if $\theta = 0$ and both
  vectors are equal, the result would be the magnitude squared (dot product of
  the vector with itself). In other words, if we take $\sqrt{\alpha}$ we see
  why the length of a vector is computed as the square root of the dot product
  of the vector with itself.
* $\theta =  180^{\circ}$, then $\cos (\theta) = -1$, and 
  $\alpha = -|\mathbf{a}||\mathbf{b}|$. The same as the previous case, just
  with a negative sign

  
~~[...] you will really be doing yourself a favor by learning [...] the
relationship between the cosine of the angle between two vectors and the sign
of the DOT product between those two vectors.~~

### Algebra vs. Geometry

$$
\alpha = \mathbf{a}^{T}\mathbf{b}
=
\underbrace{\sum^{n}_{i=1} \mathbf{a}_{i} \mathbf{b}_{i}}_{algebra}
=
\underbrace{
\cos \left( \theta_{\mathbf{ab}} \right) |\mathbf{a}| |\mathbf{b}|
}_{geometry}
$$

At first, the geometric and the algebraic formulas for the DOT product may seem completely different, but one can be derived from the other. The proof they are
equivalent relies on the law of cosines.

### The law of cosines

The law of cosines is about finding the length of a triangle's side ($C$) based
on the length of the other other two ($a, b$), and the angle between them.
When we have a non-right triangle, we can split it in two by drawing a line
($d$) perpendicular to one of the sides (e.g., $b$) that goes to the angle in
front of that side. This splits a known side into two with unknown lengths
(e.g., $b = b_1 + b_2$), and the triangle into two right triangles. The length
of the line we drew ($d$) and the side part belonging to the triangle with a
known angle can be obtained with trigonometry:

$$
\begin{aligned}
    \cos (\theta) = b_{2}/a \rightarrow b_{2} = a \cos \theta \\
    \sin (\theta) = d/a     \rightarrow d     = a \cos \theta \\
\end{aligned}
$$

The length of the $b_{1}$ is obtained as:

$$
b_{1} = b - b_{2} = b - a \cos \theta
$$

Now we can obtain the length of $c$ using the pythagorean theorem. We substitute $d$ and $b_1$, expand, and group the squared squared trigonometric terms. Then, we can simplify using the identity
$\sin (\theta)^{2} + \cos (\theta)^{2} = 1$.

$$
\begin{aligned}
c^{2} &= d^{2}+a^{2}_{1} \\
      &= a^{2} \sin(\theta)^{2} + (b - a \cos \theta)^{2} \\
      &= a^{2} \sin(\theta)^{2} + b^{2} + a^{2} \cos(\theta)^{2}
         - 2 ab \cos \theta\\
      &= a^{2} \left( \sin(\theta)^{2} + \cos(\theta)^{2} \right)
         + b^{2} - 2 ab \cos \theta\\
      &= a^{2} + b^{2} - 2 ab \cos \theta \\
\end{aligned}
$$

That's the expression of the law of cosines. In the specific case where 
$\theta = 90^{\circ}$, $\cos (\theta) = 0$, going back to the Pythagorean theorem. The Pythagorean theorem is really just a special case of the law of cosines!.

### The law of cosines as vectors

On algebraic terms the length of side $c$ would be $|\mathbf{c}|$, where
$\mathbf{c}$ is a vector. It can can be expressed as the subtraction of
$\mathbf{a}$ and $\mathbf{b}$:
$$
|\mathbf{c}|^{2} = |\mathbf{a} - \mathbf{b}|^{2}
$$

We also know that the length of a vector squared is equal to that vector dot product with itself, so:

$$
|\mathbf{a} - \mathbf{b}|^{2}
=
(\mathbf{a} - \mathbf{b})^{T}(\mathbf{a} - \mathbf{b})
$$

If we work through the multiplication we end up with:

$$
(\mathbf{a} - \mathbf{b})^{T}(\mathbf{a} - \mathbf{b})
=
\mathbf{a}^{T} \times \mathbf{a}
+
(-\mathbf{b}^{T} \times -\mathbf{b})
+
(-\mathbf{b} \times \mathbf{a}^{T})
+
(\mathbf{a}^T \times - \mathbf{b})
=
|\mathbf{a}|^{2} + |\mathbf{b}|^{2} - 2\mathbf{a}^{T}\mathbf{b}
$$

Now we can also rewrite the law of cosines:

$$
|\mathbf{a}|^{2} + |\mathbf{b}|^{2} - 2\mathbf{a}^{T}\mathbf{b}
=
|\mathbf{a}|^{2} + |\mathbf{b}|^{2}
-
2|\mathbf{a}| |\mathbf{b}| \cos \left( \theta_{\mathbf{a}\mathbf{b}} \right)
$$

So if express the magnitude squared of vector $|\mathbf{c}|$ using linear
algebra and geometry we see the following: 

$$
\begin{aligned}
|\mathbf{a} - \mathbf{b}|^{2}
&=
|\mathbf{a}|^{2} + |\mathbf{b}|^{2}
-
2|\mathbf{a}| |\mathbf{b}| \cos \left( \theta_{\mathbf{a}\mathbf{b}} \right) \\
% &=(\mathbf{a} - \mathbf{b})^{T}(\mathbf{a} - \mathbf{b}) \\
|\mathbf{a} - \mathbf{b}|^{2}
&=
|\mathbf{a}|^{2} + |\mathbf{b}|^{2} - 2\mathbf{a}^{T}\mathbf{b}
\end{aligned}
$$

The left hand side is the same in both equations, and the first and second term
of the first equation are the same as their counterparts on the second one.
That leaves the third terms, whose leading $-2$ cancel out, leaving:

$$
\mathbf{a}^{T}\mathbf{b}
=
|\mathbf{a}| |\mathbf{b}| \cos \left( \theta_{\mathbf{a}\mathbf{b}} \right) \\
$$

### Python

First we make a pair of 2D vectors.

```{python}
v1= np.array([2, 4, -3]) 
v2= np.array([0, -3, -3])
```

Then we can calculate the angle between both vectors, which involves both the
algebraic and geometric representations. It the arc cosine of the DOT product
between the vectors scaled by the product of the lengths (norms) of the two
vectors. If you pay close attention you may notice this calculation involves
both the algebraic and the geometric representation.

```{python}
angle = np.arccos(np.dot (v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))

fig = plt.figure() 
ax = fig.add_subplot(projection='3d') 
ax.plot([0, v1[0]], [0, v1[1]], [0, v1 [2]], 'b') 
ax.plot([0, v2[0]], [0, v2[1]], [0, v2[2]],'r') 
# plt.axis((-6, 6, -6, 6)) 
ax.set_title(
    f"Angle has {angle:.2f} radians, {np.rad2deg(angle):.2f} degrees")
fig.tight_layout();
```

Once the angle is known, we can see the equivalence of the algebraic and
geometric dot product formulas. 

```{python}
# algebraic
print(np.dot(v1, v2))

# geometric
print(np.linalg.norm(v1) * np.linalg.norm(v2) * np.cos(angle))
```

The result is almost the same, with some difference caused by precision
errors.

### Summary




It turns out there's tiny, tiny, tiny precision errors.

We get a tiny computer around the area.

But basically these the DOT products are equal to three in both these cases.

So in this video, you learned how to compute the angle between any two vectors of any dimensionality.

You learned the geometric interpretation of the DOT product and you learned a few different categories

are four different categories of the sign of the DOT product, depending on the angle between the two

vectors.

As I mentioned in a previous video, the DOT product is one of the most fundamental computations in

many areas of theoretical and applied linear algebra.

So it's important to be familiar with these two ways of thinking about the DOT product.






## Vector Hadamard multiplication

```python

# create vectors
w1 = [ 1, 3, 5 ]
w2 = [ 3, 4, 2 ]

w3 = np.multiply(w1,w2)
print(w3)

```

# Vector outer product

```python

v1 = np.array([  1, 2, 3 ])
v2 = np.array([ -1, 0, 1 ])

# outer product
np.outer(v1,v2)

# terrible programming, but helps conceptually:
op = np.zeros((len(v1),len(v2)))
for i in range(0,len(v1)):
    for j in range(0,len(v2)):
        op[i,j] = v1[i] * v2[j]

print(op)
```

## Vector cross product

```python
# create vectors
v1  = [ -3,  2, 5 ]
v2  = [  4, -3, 0 ]

# Python's cross-product function
v3a = np.cross( v1,v2 )

# "manual" method
v3b = [ [v1[1]*v2[2] - v1[2]*v2[1]],
        [v1[2]*v2[0] - v1[0]*v2[2]],
        [v1[0]*v2[1] - v1[1]*v2[0]] ]

print(v3a,v3b)


fig = plt.figure()
# ax = fig.add_subplot(projection='3d')
ax = plt.axes(projection = '3d')

# draw plane defined by span of v1 and v2
xx, yy = np.meshgrid(np.linspace(-10,10,10),np.linspace(-10,10,10))
z1 = (-v3a[0]*xx - v3a[1]*yy)/v3a[2]
ax.plot_surface(xx,yy,z1,alpha=.2)

## plot the two vectors
ax.plot([0, v1[0]],[0, v1[1]],[0, v1[2]],'k')
ax.plot([0, v2[0]],[0, v2[1]],[0, v2[2]],'k')
ax.plot([0, v3a[0]],[0, v3a[1]],[0, v3a[2]],'r')


ax.view_init(azim=150,elev=45)
plt.show()
```

## Hermitian transpose (a.k.a. conjugate transpose)

```python
# create a complex number
z = complex(3,4)

# magnitude
print( np.linalg.norm(z) )

# by transpose?
print( np.transpose(z)*z )

# by Hermitian transpose
print( np.transpose(z.conjugate())*z )


# complex vector
v = np.array( [ 3, 4j, 5+2j, complex(2,-5) ] )
print( v.T )
print( np.transpose(v) )
print( np.transpose(v.conjugate()) )
```

    5.0
    (-7+24j)
    (25+0j)
    [3.+0.j 0.+4.j 5.+2.j 2.-5.j]
    [3.+0.j 0.+4.j 5.+2.j 2.-5.j]
    [3.-0.j 0.-4.j 5.-2.j 2.+5.j]

## Unit vector

```python

# vector
v1 = np.array([ -3, 6 ])

# mu
mu = 1/np.linalg.norm(v1)

v1n = v1*mu

# plot them
plt.plot([0, v1n[0]],[0, v1n[1]],'r',label='v1-norm',linewidth=5)
plt.plot([0, v1[0]],[0, v1[1]],'b',label='v1')

# axis square
plt.axis('square')
plt.axis(( -6, 6, -6, 6 ))
plt.grid()
plt.legend()
plt.show()
```

## Span

```python
# set S
S1 = np.array([1, 1, 0])
S2 = np.array([1, 7, 0])

# vectors v and w
v = np.array([1, 2, 0])
w = np.array([3, 2, 1])

# draw vectors
fig = plt.figure()
# ax = fig.add_subplot(projection='3d')
ax = plt.axes(projection = '3d')
ax.plot([0, S1[0]],[0, S1[1]],[.1, S1[2]+.1],'r',linewidth=3)
ax.plot([0, S2[0]],[0, S2[1]],[.1, S2[2]+.1],'r',linewidth=3)

ax.plot([0, v[0]],[0, v[1]],[.1, v[2]+.1],'g',linewidth=3)
ax.plot([0, w[0]],[0, w[1]],[0, w[2]],'b')

# now draw plane
xx, yy = np.meshgrid(range(-15,16), range(-15,16))
cp = np.cross(S1,S2)
z1 = (-cp[0]*xx - cp[1]*yy)*1./cp[2]
ax.plot_surface(xx,yy,z1)

plt.show()
```
