# Vectors

```{python}
import numpy as np
import matplotlib.pyplot as plt
```


## Algebraic and geometric interpretations

Many concepts in linear algebra can be conceptualized using both algebra and
geometry. This is convenient because some concepts are more intuitive from one
of these two points of view.

A vector is an ordered list of numbers like this. Their representation generally
is surrounded by brackets (sometimes angled brackets, or parentheses). Vectors
contain several elements, numbers of any kind: integers or fractions,
real or complex, rational or irrational. 

$$
\begin{bmatrix}
1 \\ 2 \\ 3
\end{bmatrix}
\neq
\begin{bmatrix}
2 \\ 1 \\ 3
\end{bmatrix}
$${#eq-vec-order}

The number of elements in a vector is called the dimensionality of the vector.
In @eq-vec-order, both sides are three dimensional vectors. They are not the
same because the order of these elements is meaningful: the first element on
the left side is $1$, whereas on the right side it is $2$. @eq-vec-size shows
a 2- and 5-dimensional vectors.

$$
\begin{bmatrix}
3 \\ 4
\end{bmatrix}
\quad
\begin{bmatrix}
1 \\ 0 \\ \pi \\ 5i \\ -2
\end{bmatrix}
$${#eq-vec-size}

Vectors can be vertical, case where they are called column vectors, or
horizontal, case where they are called row vectors.

$$
\begin{bmatrix}
1 & 2 & 3 & 5 & 4 \\
\end{bmatrix}
$$

Nomenclature around vectors is a bit confusing because source it uses one of
many conventions. The most common way to refer to vectors is using a lowercase,
bold Latin letter, such as $\mathbf{v}$. Sometimes you'll see a little arrow on top of
the letter $\left( \vec{\mathbf{v}} \right)$, which is used to avoid ambiguity about
whether or not this is referring to a vector. Rarely you may encounter vectors
represented using italicized letters. Here we just use lowercase boldface
letters $\left( \mathbf{v} \right)$.

### Geometric interpretation

In geometric terms a vector represents a straight line with some length and
some direction, which is specified by the numbers contained in the vector.
The starting point of vector is called the tail of the vector, and its end is
called the head, symbolized by an arrowhead. 

Note a vector is defined just by its length and direction respective from the
start: it these could start from any point in space. In other words, vectors
are robust to translation Â¿or rigid body motion?. Thus, even though it may be
tempting to interpret the contents of a 2D vector as a coordinate, this only
holds if the tail of the vector is placed on the origin: the vector and the
coordinates are overlapping. If a vector starts at $(0, 0)$ they are considered
to be in standard position. Such position is convenient for many reasons (it
can be assumed if nothing has stated otherwise).

A vector can be 2-, 3-dimensional, etc. Even though past this point
visualization becomes challenging, the principle remains the same.
It also shows a common theme in linear algebra: the geometric interpretation
is useful up to 3 dimensions, point where is more intuitive to switch back
to algebra.

Note vectors can also contain functions.

$$
\begin{bmatrix}
\sin (x) \\
x \cos(x) \\
x \\
\end{bmatrix}
$${#eq-fun-vector}

@eq-fun-vector is a vector valued function where $x$ is some continuous vector.
However, in this course, I'm only going to be talking about vectors where each
element is a number, not an entire function.

### Python

We will be using Numpy, Matplotlib and their submodules (pyplot) for all the
lessons in this course. A list could be considered a 2D vector, but in most
cases is preferable to use a Numpy array, specially for complex cases.
Transpose swaps, the rows and the columns. This operation can be called as a
function, or as a method of Numpy arrays (`.T`).

When we use the function `plt.plot` we are just specifying the $x$ and the $y$
values to draw a line. The code indicates the line start at the origin
$(0, 0)$, and ends at the $x$, $y$ locations corresponding to the first and
second elements of the vector, respectively. So the $x$ coordinate is at $3$
and the $y$ coordinate is $-2$. The rest of the code sets the aspect of the
figure, adds the cartesian axis lines, the grid and sets the plot extent.

```{python}

# 2-dimensional vector
v2 = [ 3, -2 ]

# 3-dimensional vector
v3 = [ 4, -3, 2 ]

# row to column (or vice-versa):
v3t = np.transpose(v3)


# plot them
plt.plot([0,v2[0]],[0,v2[1]])
plt.axis('equal')
plt.plot([-4, 4],[0, 0],'k--')
plt.plot([0, 0],[-4, 4],'k--')
plt.grid()
plt.axis((-4, 4, -4, 4))
```

3D plotting in Python requires a few actual pieces of code compared to plotting
in 2D. In this case, `ax.plot` is a method of a figure panel (`ax` in
Matplotlib terminology) which has been been set to be projected in 3D.
The first argument are the $x$ coordinates, the second the $y$ coordinates,
ad the third the $z$ coordinates. The rest of the code is just setting the axis,
the extent, etc.

```{python}

# plot the 3D vector
fig = plt.figure(figsize=plt.figaspect(1))
# ax = fig.add_subplot(projection='3d')
ax = plt.axes(projection = '3d')
ax.plot([0, v3[0]],[0, v3[1]],[0, v3[2]],linewidth=3)

# make the plot look nicer
ax.plot([0, 0],[0, 0],[-4, 4],'k--')
ax.plot([0, 0],[-4, 4],[0, 0],'k--')
ax.plot([-4, 4],[0, 0],[0, 0],'k--')
plt.show()
```

### Summary

* Algebraic and the geometric interpretation of a vector
* The difference between a vector and a coordinate and the one case
* The special case where the vector and the coordinate overlap when the vector
*  is being drawn in its standard position.

## Vector addition/subtraction

When doing operations with vectors it is necessary they have the same number of
elements, the same dimensionality.

### Algebraic interpretation

Algebraic addition or subtraction just apply the operation to each
pair of elements.

$$
\begin{bmatrix}
1 \\ 0 \\ 4 \\ 3
\end{bmatrix}
+
\begin{bmatrix}
2 \\ -3 \\ -2 \\ 1
\end{bmatrix}
=
\begin{bmatrix}
3 \\ -3 \\ 2 \\ 4
\end{bmatrix}
$$

### Geometric interpretation

#### Addition

Geometrically, to add two vectors, you put the tail of one vector at the head
of the other (remember, vectors are about length and orientation, not coordinates).
And just draw the line that goes from the tail of the first vector to the head
of the second.

#### Subtraction

Geometrically, vector subtraction has two interpretations.

You can think of subtraction as multiplying the second vector by $-1$, element
by element, and then adding it to the first vector. This flips the direction of
the vector.

The second way to think about vector subtraction is to put both vectors into
the standard position, and then you draw a straight line going from the head of
the negative negative vector ("$-$" sign) to the head of the positive vector.

In both cases the resulting vector could be translated back to the standard position.

In the PDF file there are more exercises so you can get more comfortable with
vector addition and subtraction, both from the algebraic and the geometric
perspectives.

**translated vector V2 so that it starts at the head of Vector V1**

### Python

Vector addition and subtraction requires the use Numpy arrays (lists are
concatenated with the plus sign). They can added or subtracted with just the
`+` or the `-` sign. You can think about the addition of two vectors as going
from the tail of one vector to the head of the other vector when that vector
is translated to be at the end of the first vector.

```{python}

# two vectors in R2
v1 = np.array([ 3, -1 ])
v2 = np.array([ 2,  4 ])

v3 = v1 + v2

# plot them
plt.plot([0, v1[0]],[0, v1[1]],'b',label='v1')
plt.plot([0, v2[0]]+v1[0],[0, v2[1]]+v1[1],'r',label='v2')
plt.plot([0, v3[0]],[0, v3[1]],'k',label='v1+v2')

plt.legend()
plt.axis('square')
plt.axis((-6, 6, -6, 6))
plt.grid()
plt.show()
```

## Vector-scalar multiplication

A scalar is just a single number outside of a vector or a matrix.

*And you'll see in a few minutes why single numbers in linear algebra are called scalars.*

In linear algebra symbology vectors are represented with boldfaced lowercase
Latin letters (e.g., $\mathbf{v}$), matrices with capital boldface letters
(e.g., $\mathbf{M}$), and scalars with lowercase non boldfaced Greek letters
(e.g., $\alpha$). Note this convention is not followed all the time.

$$
\textbf{v}
=
\begin{bmatrix}
-1 \\ 0 \\ 1
\end{bmatrix}
\quad
\textbf{M}
=
\begin{bmatrix}
-1 & 0 & 2 & 8 \\
0 & 1 & 4 & 4 \\
1 & 4 & 9 & 1 \\
\end{bmatrix}
\quad
\alpha = 7
$$

Vector scalar multiplication has both an algebraic and a geometric interpretation.

### Algebraic

Algebraically vector scalar multiplication means to multiply each element by
the scalar.

$$
\lambda \mathbf{v}
=
7
\begin{bmatrix}
-1 \\ 0 \\ 1
\end{bmatrix}
=
\begin{bmatrix}
7 \times -1 \\
7 \times 0 \\
7 \times 1
\end{bmatrix}
=
\begin{bmatrix}
-7 \\ 0 \\ 7
\end{bmatrix}
$$

### Geometric

The geometric interpretation of vector scalar multiplication is to "scale"
the vector according to the scale (no change in direction). If the scalar is
greater than one ($\lambda > 1$), the resulting vector will be longer than the
original, whereas if it is $\lambda \in (0, 1)$, the result will be shorter.
If the scalar is negative ($\lambda < 0$) spins around and points in the other
way (same direction, opposite orientation).

*[The vector line] is actually lying on a one dimensional subspace that goes
infinitely long in [either] direction. And in that sense, which is a more
linear algebra sense, this vector is not actually pointing in a different
direction with regards to the one dimensional subspace, the infinitely long
line that this line or this vector is on. If scaling a vector doesn't change
its angle, that means that all vectors obtaining by scaling one particular
vector are related to each other in a special way: they lie on the same
subspace (see later).*

### Python 

We create a 2D vector `v1` and multiply by `l`, standing for lambda, $\lambda$.
`l = -.3` is negative and its magnitude is smaller than one, which means the
result will be shorter than the original and have the opposite orientation even
though the direction remains the same. with `l = 2.3` the scaled will be longer
and follow the same orientation as the original one.

The original vector is plotted in blue and the scaled version is plotted in red.
The auxiliary variable `axlim` is used to adjust the extent of the plot
according to largest elements in the original and the scaled vectors.

```python
# vector and scalar
v1 = np.array([ 3, -1 ])
l  = 2.3
v1m = v1*l # scalar-modulated

# plot them
plt.plot([0, v1[0]],[0, v1[1]],'b',label='$v_1$')
plt.plot([0, v1m[0]],[0, v1m[1]],'r:',label='$\lambda v_1$')

plt.legend()
plt.axis('square')
axlim = max([max(abs(v1)),max(abs(v1m))])*1.5 # dynamic axis lim
plt.axis((-axlim,axlim,-axlim,axlim))
plt.grid()
plt.show()
```

### Summary

Multiplying a vector by a scalar changes the length but preserves the direction
of that vector (not the orientation). It may seem like trivial operation, but
this idea turns out to be the fundamental insight that leads to eigenvalue
decomposition, one of the most important applications of linear algebra.

# Vector-vector multiplication: the dot product

There are four ways to multiply two vectors:

1. Had Ahmad multiplication, t
2. The dot product, 
3. the cross product
4. the outer product

The dot product is a single number that provides information about the
relationship between two vectors with the same dimensionality. Sometimes it
also is called the scalar product because the result is a single number.
The dot product is the computational cornerstone of many algorithms, such as
convolution, correlation or the Fourier transform.

### Algebraic interpretation (only)

The DOT product is indicated using several different notations.

$$
\alpha
= \mathbf{a} Â· \mathbf{b} = \langle \mathbf{a}, \mathbf{b} \rangle
= \mathbf{a}^{T} \mathbf{b} = \sum^{n}_{i=1} a_i b_i
$$

The dot product is a scalar, so you can refer to its result using a lowercase
greek letters. You also may see a dot between the two vectors or wide angle
brackets. The latter is more commonly used for continuous functions, but
sometimes it is used used for the dot product. $a^{T} b$ notation is the most
common in linear algebra: the first vector transpose $(T)$ times, the second
vector.

$\mathbf{a}^T \mathbf{b}$ indicates the vector dot product assuming
$\mathbf{a}$ and $\mathbf{b}$ are both column vectors. The algebraic
definition of the dot product is just element-wise and sum the results.
Let me unpack this formula by showing you an example. @eq-dot-ew shows the
dot product between two five-dimensional column vectors, $\mathbf{v}$ and
$\mathbf{v}$. The dot product between these two vectors is element-wise
multiplication and then sum.

$$
v=
\begin{bmatrix}
1 \\ 0 \\ 2 \\ 5 \\ -2
\end{bmatrix}
\quad
w=
\begin{bmatrix}
2 \\ 8 \\ -6 \\ 1 \\ 0
\end{bmatrix}
\quad
\mathbf{v}^T \mathbf{w} =
1 \times 2 + 0 \times 8 + 2 \times (-6) + 5 \times 1 + (-2) \times 0 = -5
$${#eq-dot-ew}

### Python

The first step is to create the vectors `v1`and `v2`.

```{python}
v1 = np.array([ 1, 2, 3, 4, 5 ]) # , 6
v2 = np.array([ 0, -4, -3, 6, 5 ])
```

On the barest terms, we could  loop through the elements of the vectors,
multiply the $i^{th}$ element of the vectors, and add to a variable initialized
at $0$.

```{python}
the_sum = 0
for i in range(len(v1)):
    the_sum += v1[i] * v2[i]
print(the_sum)
```

This code can be reduced a bit using Numpy's vectorized operations

```{python}
print(np.sum(np.multiply(v1,v2)), np.sum(v1 * v2))
```

The library also has the specialized function `np.dot` to calculate the dot
product.

```{python}
print(np.dot(v1, v2))
```

It is also possible to calculate the dot product using matrix multiplication
(see next sections).

```{python}
print(np.matmul( v1,v2 ), v1 @ v2)
```

Even though the result is $32$ in all cases, in practice is advisable to use
`np.dot` function for convenience, clarity and speed.

# VIDEO: Dot product properties: associative and distributive


```python
## Distributive property

# create random vectors
n = 10
a = np.random.randn(n)
b = np.random.randn(n)
c = np.random.randn(n)

# the two results
res1 = np.dot( a , (b+c) )
res2 = np.dot(a,b) + np.dot(a,c)

# compare them
print([ res1,res2 ])
```


```python
## Associative property

# create random vectors
n = 5
a = np.random.randn(n)
b = np.random.randn(n)
c = np.random.randn(n)

# the two results
res1 = np.dot( a , np.dot(b,c) )
res2 = np.dot( np.dot(a,b) , c )

# compare them
print(res1)
print(res2)


### special cases where associative property works!
# 1) one vector is the zeros vector
# 2) a==b==c

```

The goal of this video is to explore the associative property and the
distributive properties of the dot product.

### The distributive property.

When working with scalars the distributive property means that $a(b+c)$ is the
same thing as $ab + ac$

$$
a (b + c) = ab + bc
$$

When working with vectors

$$
\mathbf{a}^{T}(\mathbf{b} + \mathbf{c})
=
\mathbf{a}^{T}\mathbf{b} + \mathbf{a}^{T}\mathbf{c}
$$

Thus, the vector dot product is distributive.

To explain why that is the case, I'm going to actually start from the
rightmost term and rewrite its two DOT products.

$$
\sum^{n}_{i=1} a_i b_i + \sum^{n}_{i=1} a_i c_i
=
\sum^{n}_{i=1} a_i (b_i + c_i)
$$

Using the algebraic definition, is the element-wise multiplication.
So we take elements $i$ to $n$ (dimensionality of the vectors) from each vector,
multiply one another, and sum. All three vectors have the same dimensionality
and all the operations are linear (multiplication/summation).
Thus, it is possible to combine the Â¿like? terms and rewrite:
the vector dot product is distributive.

### Associative property

Now, I'd like to get to the associative property.

And again, just as a brief reminder, this is not linear algebra.

This is kind of standard high school scalar algebra.

The associative property basically just means that you can put parentheses wherever you want if you

have a bunch of multiplication.

So the number A times the quantity B times C is the same thing as the quantity eight times B and then

that business times C.

So really all we're doing is moving these parentheses around.

So scalar multiplication is associative, but the vector dot product is not associative.

So that means that a transpose times the quantity.

B transpose times C is in general not the same thing as the quantity.

A transpose times B and then that quantity transpose times C.

Now at this point I encourage you to pause the video in order to come up with two things on your own.

First is a proof or a reason why this inequality is the case.

And secondly, I would also like you to come up with one or two special cases where this equality actually

does hold.

So there are some special cases you can find vectors A, B and C where the associative property will

hold.

But of course, those are a few rare special cases.

In general, the associative property does not hold for the vector dot product.

I'm going to give you two reasons why this equality is not true.

So why the DOT product is not associative.

And to start with, let's assume that A, B and C are all distinct vectors.

They're all different from each other and none of these vectors is the Xeros vector.

So none of these vectors is just a vector of all zeros.

Now, notice that B, transpose C and A transpose B are both scalars, these work out to be just individual

numbers.

And so assuming that A, B and C are all unique, distinct vectors, then B Transpose C is certainly

not going to be the same thing as A transpose B.

Or at least we can expect them to be different from each other.

Furthermore, we can also expect A and C to be different, or at least I said that they're different.

And so here you have two distinct vectors multiplying, two distinct scalars.

So, of course, the result is not going to be the same.

In fact, they're not even going to have the same orientation.

This side is going to produce a row vector and this side is going to produce a column vector.

So that is one explanation for why the DOT product is not associative.

Another explanation comes from just thinking about dimensionality of these different vectors.

So let's assume, for example, that A is some vector in R5.

So those five elements and vectors, B and C are in our three.

So they each have three elements.

And now it turns out that a transpose times the quantity or I should say the DOT product between B and

C, this is a totally valid expression.

There's nothing wrong with this expression.

In fact, this just works out to be scalar vector multiplication.

On the other hand, taking exactly these same vectors and exactly the same orientations, just moving

around the parentheses completely changes.

And now this is no longer even a valid multiplication.

You're trying to compute the product between a five dimensional vector and a three dimensional vector.

So that's even worse than having this inequality here.

One side of the inequality is not even a valid operation.

OK, so the vector dot product is not associative, but if things get a little bit confusing, because

when you start learning about matrix multiplication in the next several sections of the course, you

will see that in fact, matrix multiplication is associative.

So if A, B and C are all matrices, then eight times the quantity, B times C is the same thing as

the quantity, eight times B and then all of that time C..

Now I realize I haven't told you yet about matrices and matrix multiplications, so don't worry about

trying to understand why this is the case.

The main thing I want to convey to you with this slide is that you shouldn't think that somehow the

associative property is never valid in linear algebra.

It's just for the DOT product where it's invalid.

In fact, this associative property for matrix multiplications turns out to be really crucial for quite

a few proofs in advanced linear algebra.

And you will see that throughout this course.

OK, so so far in this video, I've told you about two properties of the product, the associative property

and the distributive property, and you might be wondering whether the product is also commutative,

whether the commutative property also applies to the vector dot product.

And the answer is, I'm not going to tell you.

And that's because there's a code challenge in a few videos later on in this section where you will

have the opportunity to discover yourself whether the vector dot product is, in fact, commutative.

All right.

So what I'd like to do now is switch to Matlab and show you these two algebraic properties in code.

I will begin here by showing you the distributive property of the vector dot product.

So here I create three random vectors.

I call them A, B and C, and they are in R 10.

So they are 10 dimensional or 10 element vectors.

And well, the number of the dimensionality of these three vectors doesn't actually matter for the distributive

property, although of course it is important that all three of them are in the same dimensionality.

So you could change this number as long as they all have the same.

And here.

OK, so let's create our vectors.

And then we have our two results here.

So we have A transpose times B plus C, and then over here in Variable Redd's two, we have basically

the distributed version of this.

So A transpose times B plus A transpose time C, and now if these two operations are the same, then

these two outcomes should be the same.

So if we find that these two that Rezwan and these two are the same numbers, then that provides some

evidence for the distributive property.

That's an illustration of the distributive property.

So here you see, I ran this code and here we get the result.

And it doesn't actually matter what these two numbers are, it just matters that they are the same.

Obviously, these two numbers are going to be different every time I rerun the code because they're

just randomly generated vectors.

But every time I run the code, we get exactly the same result for a transpose times B plus C and A

transpose B plus A transpose C.

OK, so that's an illustration of the distributive property.

Now let's go to the associative property and see how this works out.

OK, so again, we start off by creating some random vectors and they're all in the same dimensionality.

So they're all are 10 vectors.

And now here we are going to compute again our two possible results.

And then well, I already explained in the slides in theory why these two results are not going to be

the same.

So now let's see how this looks in practice.

So here we have a transpose times the product or the product of B transpose times.

Now, remember, this thing is actually a vector.

This is not sorry, this is a scalar.

These are two vectors.

The product here ends up being a scalar.

So it's a little bit you know, it's it's terminological, questionable whether you could actually call

this thing a DOT product because the DOT product is, you know, Element Y's multiplication and some

between two vectors.

But this is a vector.

This is no longer a vector.

So, you know, it's an interesting discussion.

But for now, I'm going to stick to the question of, you know, whether this is a valid operation.

This is a valid mathematical operation.

And is it the same as this, which is also a valid mathematical operation?

OK, so it turns out that if A, B and C are all the same dimensionality, then these are both individually

valid operations.

We don't get any errors here.

So there's nothing really wrong with the math.

However, these two results are not going to be the same.

And in fact, they're not even the same orientation as I already mentioned in the slide.

So I even have to transpose this first result just to be able to display them side by side like this.

And it's pretty clear that these are not the same result.

And, you know, as I also mentioned a moment ago, these are also no longer dot products.

These are vector scalar multiplications.

OK, so let's see.

So that means that the associative property does not hold.

Now, I also mentioned in the slides that we can have these even stranger situations here where one

of these expressions is mathematically possible and the other one is not defined.

And that will be the case when these are in different dimensionality.

So let's have A and B, B, still 10 dimensional vectors and now C is going to be a 14 dimensional vector.

So now let's see what happens when we try to compute as one and Redd's two.

So it turns out that this operation is not even valid.

And that's because we are trying to compute the DOT product between a vector in our ten with a vector

in our 14.

So that is illegal.

I have to watch my back in case the linear algebra police are going to come knocking down at my door.

OK, but in contrast to actually is a valid result and this is a valid operation because now we are

computing the DOT product between A and B and those are those have the same dimensionality and then

this is just a scalar.

So that ends up being multiplied by vector C.

And so, yeah, there's there's nothing particularly weird or wrong about this operation, although

you wouldn't call it a DOT product.

Nonetheless, this is a demonstration that the associative property does not hold.

For Vector's, OK, and now here I want to show two special cases where the associative property actually

does hold.

Now, I mentioned in in the slides that there are a few special cases where the associative property

holds, and here I'm listing two of them.

So one special case is when one of these vectors is the zeroes vector.

Now, they all do have to be the same dimensionality as I just demonstrated to you.

But it turns out that if one of these vectors is the Xeros vector, then Rezwan and these two are going

to be the same up to a transpose.

So they're not going to be identical in orientation, but the values are going to be the same.

But, you know, that's actually kind of trivial because, you know, basically we're just multiplying

stuff by a lot of zeros.

So, of course, we always get zeros no matter what we do, but that is technically a special case.

OK, and then the other case, which I'm not going to demonstrate here, but I encourage you to pause

the video and take a moment to confirm this to yourself, that when the vectors are all equal, so A

is the same thing as B is the same thing as see them at the associative property is still going to work.

That's another special case where the associative property works.

Again, it is debatable whether this thing would be called a DOT product, but we will get these two

results to be the same.

So.

So please take a moment and code this up yourself to confirm that that's true.

Now, turns out these are not the only two special cases.

So if you think you can come up with another special case, then go for it.

I'm going to start here by illustrating the distributive property, and I'm going to do that by creating

10 element vectors.

These are all random numbers vectors.

So these are vectors with 10 elements, each of which so each element is drawn from a normal distribution

of random numbers.

So just a bunch of random numbers.

And then here's basically the two key computations that we doing here.

So we take the DOT product between Vector A. and vectors B plus C, and then we take the DOT product

between A and B and the DOT product between A and C. So you can see how we are distributing the DOT

product from A to B. That's here and A to C is here and here is A with B plus C..

OK, and then we're going to print out these two results and then basically we want to see that these

give us the same numbers, which they certainly do.

So these are the same values and that is an illustration of the distributive property.

Now, of course, because these are randomly generated vectors, every time I rerun this, the numbers

are going to be the same.

And of course, they're going to be different.

I'm sorry, the numbers are different each time I run it and they're going to be different for you.

The important thing is that these two numbers are the same as each other.

That's the important thing here.

All right.

So now we get to the associative property and I'm just reducing the end here.

It's just basically to make it easier to print out.

So what we're doing, again, is creating new vectors here.

And then we compute the DOT product between Vector A and the DOT product of B and C..

Now, this is a little bit funny notation.

There's also discussion in the Q&A about this because technically this is not really the DOT product

anymore.

This becomes vector scalar multiplication, but it sort of gets implemented as the DOT product.

So the DOT product between Vector A and the scalar, which results from the DOT product of B and C.

So this is just the easiest way to implement what I showed in the slides.

And then here we're going to compare these two methods and see if they are the same and if these two

vectors, these two results are the same.

And that would provide evidence for the associative property holding in vectors.

And you can see that these numbers do not match.

They're not even close.

We can run it over and over again.

It's not just some weird fluke.

They never match.

And that's because the associative property does not actually hold.

That is to say, the associative property does not hold.

In general, there are special cases where the associative property does work.

One of those case are two of those cases are listed here where one vector is the Xeros vector.

It's kind of trivial.

You multiply it by the zeros vector and you're just going to get a bunch of zeros.

And another special case is where all of the vectors are equal to each other.

So in A equals B equals C, then it turns out that the associative property also holds for this special

case.

Now, I'm not going to illustrate that to you here in code, but I encourage you to take a moment and

try both of these and test them out for yourself and see whether that actually works.

And maybe you can also discover some other special cases, because this is not an exclusive list here.

So in this video, I showed you that the vector dot product obeys the distributive property, but not

the associative property, and we left a little bit of mystery, a little bit of uncertainty about whether

the DOT product is also commutative, you're going to have to wait a few videos to discover that on

your own.

---
# VIDEO: Vector length
---



```python
# a vector
v1 = np.array([ 1, 2, 3, 4, 5, 6 ])

# methods 1-4, just like with the regular dot product, e.g.:
vl1 = np.sqrt( sum( np.multiply(v1,v1)) )

# method 5: take the norm
vl2 = np.linalg.norm(v1)

print(vl1,vl2)
```


---
# VIDEO: The dot product from a geometric perspective
---



```python

# two vectors
v1 = np.array([ 2,  4, -3 ])
v2 = np.array([ 0, -3, -3 ])

# compute the angle (radians) between two vectors
ang = np.arccos( np.dot(v1,v2) / (np.linalg.norm(v1)*np.linalg.norm(v2)) )


# draw them
fig = plt.figure()
# ax = fig.add_subplot(projection='3d')
ax = plt.axes(projection = '3d')
ax.plot([0, v1[0]],[0, v1[1]],[0, v1[2]],'b')
ax.plot([0, v2[0]],[0, v2[1]],[0, v2[2]],'r')

plt.axis((-6, 6, -6, 6))
plt.title('Angle between vectors: %s rad.' %ang)
plt.show()

```


```python
## equivalence of algebraic and geometric dot product formulas

# two vectors
v1 = np.array([ 2,  4, -3 ])
v2 = np.array([ 0, -3, -3 ])


# algebraic
dp_a = np.dot( v1,v2 )

# geometric
dp_g = np.linalg.norm(v1)*np.linalg.norm(v2)*np.cos(ang)

# print dot product to command
print(dp_a)
print(dp_g)

```


---
# VIDEO: Vector Hadamard multiplication
---



```python

# create vectors
w1 = [ 1, 3, 5 ]
w2 = [ 3, 4, 2 ]

w3 = np.multiply(w1,w2)
print(w3)

```


---
# VIDEO: Vector outer product
---



```python

v1 = np.array([  1, 2, 3 ])
v2 = np.array([ -1, 0, 1 ])

# outer product
np.outer(v1,v2)

# terrible programming, but helps conceptually:
op = np.zeros((len(v1),len(v2)))
for i in range(0,len(v1)):
    for j in range(0,len(v2)):
        op[i,j] = v1[i] * v2[j]

print(op)
```


---
# VIDEO: Vector cross product
---



```python
# create vectors
v1  = [ -3,  2, 5 ]
v2  = [  4, -3, 0 ]

# Python's cross-product function
v3a = np.cross( v1,v2 )

# "manual" method
v3b = [ [v1[1]*v2[2] - v1[2]*v2[1]],
        [v1[2]*v2[0] - v1[0]*v2[2]],
        [v1[0]*v2[1] - v1[1]*v2[0]] ]

print(v3a,v3b)


fig = plt.figure()
# ax = fig.add_subplot(projection='3d')
ax = plt.axes(projection = '3d')

# draw plane defined by span of v1 and v2
xx, yy = np.meshgrid(np.linspace(-10,10,10),np.linspace(-10,10,10))
z1 = (-v3a[0]*xx - v3a[1]*yy)/v3a[2]
ax.plot_surface(xx,yy,z1,alpha=.2)

## plot the two vectors
ax.plot([0, v1[0]],[0, v1[1]],[0, v1[2]],'k')
ax.plot([0, v2[0]],[0, v2[1]],[0, v2[2]],'k')
ax.plot([0, v3a[0]],[0, v3a[1]],[0, v3a[2]],'r')


ax.view_init(azim=150,elev=45)
plt.show()
```


---
# VIDEO: Hermitian transpose (a.k.a. conjugate transpose)
---



```python
# create a complex number
z = complex(3,4)

# magnitude
print( np.linalg.norm(z) )

# by transpose?
print( np.transpose(z)*z )

# by Hermitian transpose
print( np.transpose(z.conjugate())*z )


# complex vector
v = np.array( [ 3, 4j, 5+2j, complex(2,-5) ] )
print( v.T )
print( np.transpose(v) )
print( np.transpose(v.conjugate()) )
```

    5.0
    (-7+24j)
    (25+0j)
    [3.+0.j 0.+4.j 5.+2.j 2.-5.j]
    [3.+0.j 0.+4.j 5.+2.j 2.-5.j]
    [3.-0.j 0.-4.j 5.-2.j 2.+5.j]



---
# VIDEO: Unit vector
---



```python

# vector
v1 = np.array([ -3, 6 ])

# mu
mu = 1/np.linalg.norm(v1)

v1n = v1*mu

# plot them
plt.plot([0, v1n[0]],[0, v1n[1]],'r',label='v1-norm',linewidth=5)
plt.plot([0, v1[0]],[0, v1[1]],'b',label='v1')

# axis square
plt.axis('square')
plt.axis(( -6, 6, -6, 6 ))
plt.grid()
plt.legend()
plt.show()
```


---
# VIDEO: Span
---



```python
# set S
S1 = np.array([1, 1, 0])
S2 = np.array([1, 7, 0])

# vectors v and w
v = np.array([1, 2, 0])
w = np.array([3, 2, 1])

# draw vectors
fig = plt.figure()
# ax = fig.add_subplot(projection='3d')
ax = plt.axes(projection = '3d')
ax.plot([0, S1[0]],[0, S1[1]],[.1, S1[2]+.1],'r',linewidth=3)
ax.plot([0, S2[0]],[0, S2[1]],[.1, S2[2]+.1],'r',linewidth=3)

ax.plot([0, v[0]],[0, v[1]],[.1, v[2]+.1],'g',linewidth=3)
ax.plot([0, w[0]],[0, w[1]],[0, w[2]],'b')

# now draw plane
xx, yy = np.meshgrid(range(-15,16), range(-15,16))
cp = np.cross(S1,S2)
z1 = (-cp[0]*xx - cp[1]*yy)*1./cp[2]
ax.plot_surface(xx,yy,z1)

plt.show()
```
